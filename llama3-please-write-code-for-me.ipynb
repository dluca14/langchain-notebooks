{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c4765ad",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.005712,
     "end_time": "2024-04-21T20:40:56.295944",
     "exception": false,
     "start_time": "2024-04-21T20:40:56.290232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "We are using LLama3 to write code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd20c9b",
   "metadata": {
    "papermill": {
     "duration": 0.004498,
     "end_time": "2024-04-21T20:40:56.305831",
     "exception": false,
     "start_time": "2024-04-21T20:40:56.301333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "id": "0eef7613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T20:40:56.316887Z",
     "iopub.status.busy": "2024-04-21T20:40:56.316048Z",
     "iopub.status.idle": "2024-04-21T20:41:01.478514Z",
     "shell.execute_reply": "2024-04-21T20:41:01.477744Z"
    },
    "papermill": {
     "duration": 5.170428,
     "end_time": "2024-04-21T20:41:01.480815",
     "exception": false,
     "start_time": "2024-04-21T20:40:56.310387",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-26T07:27:06.080174Z",
     "start_time": "2024-04-26T07:27:04.842949Z"
    }
   },
   "source": [
    "from time import time\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, Markdown"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "bab7b8d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T20:41:01.491950Z",
     "iopub.status.busy": "2024-04-21T20:41:01.491547Z",
     "iopub.status.idle": "2024-04-21T20:43:07.763415Z",
     "shell.execute_reply": "2024-04-21T20:43:07.762332Z"
    },
    "papermill": {
     "duration": 126.279476,
     "end_time": "2024-04-21T20:43:07.765543",
     "exception": false,
     "start_time": "2024-04-21T20:41:01.486067",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-26T07:27:12.262340Z",
     "start_time": "2024-04-26T07:27:09.149063Z"
    }
   },
   "source": [
    "model = \"/kaggle/input/llama-4/transformers/8b-chat-hf/1\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of '/kaggle/input/llama-4/transformers/8b-chat-hf/1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/kaggle/input/llama-4/transformers/8b-chat-hf/1' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHFValidationError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/Workspace/langchain-notebooks/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:675\u001B[0m, in \u001B[0;36mPretrainedConfig._get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfiguration_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    680\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    682\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    683\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    684\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    687\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    688\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    689\u001B[0m     commit_hash \u001B[38;5;241m=\u001B[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
      "File \u001B[0;32m~/Workspace/langchain-notebooks/venv/lib/python3.10/site-packages/transformers/utils/hub.py:429\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    428\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 429\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    435\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    436\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    437\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    438\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    440\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    441\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    442\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Workspace/langchain-notebooks/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:111\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m arg_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrepo_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 111\u001B[0m     \u001B[43mvalidate_repo_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m arg_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m arg_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Workspace/langchain-notebooks/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:159\u001B[0m, in \u001B[0;36mvalidate_repo_id\u001B[0;34m(repo_id)\u001B[0m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m repo_id\u001B[38;5;241m.\u001B[39mcount(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 159\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[1;32m    160\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepo id must be in the form \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrepo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnamespace/repo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    161\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Use `repo_type` argument if needed.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    162\u001B[0m     )\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m REPO_ID_REGEX\u001B[38;5;241m.\u001B[39mmatch(repo_id):\n",
      "\u001B[0;31mHFValidationError\u001B[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/llama-4/transformers/8b-chat-hf/1'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/kaggle/input/llama-4/transformers/8b-chat-hf/1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m \u001B[43mtransformers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext-generation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat16\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/langchain-notebooks/venv/lib/python3.10/site-packages/transformers/pipelines/__init__.py:741\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[1;32m    738\u001B[0m                 adapter_config \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m    739\u001B[0m                 model \u001B[38;5;241m=\u001B[39m adapter_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbase_model_name_or_path\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m--> 741\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[43mAutoConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_from_pipeline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    742\u001B[0m     hub_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39m_commit_hash\n\u001B[1;32m    744\u001B[0m custom_tasks \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/Workspace/langchain-notebooks/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1023\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m   1019\u001B[0m revision \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrevision\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m   1021\u001B[0m revision \u001B[38;5;241m=\u001B[39m sanitize_code_revision(pretrained_model_name_or_path, revision, trust_remote_code)\n\u001B[0;32m-> 1023\u001B[0m config_dict, unused_kwargs \u001B[38;5;241m=\u001B[39m \u001B[43mPretrainedConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_config_dict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m   1025\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1026\u001B[0m has_remote_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutoConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1027\u001B[0m has_local_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m CONFIG_MAPPING\n",
      "File \u001B[0;32m~/Workspace/langchain-notebooks/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:620\u001B[0m, in \u001B[0;36mPretrainedConfig.get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    618\u001B[0m original_kwargs \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(kwargs)\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Get config dict associated with the base config file\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m config_dict, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_config_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict:\n\u001B[1;32m    622\u001B[0m     original_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Workspace/langchain-notebooks/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:696\u001B[0m, in \u001B[0;36mPretrainedConfig._get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    693\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    694\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    695\u001B[0m         \u001B[38;5;66;03m# For any other exception, we throw a generic error.\u001B[39;00m\n\u001B[0;32m--> 696\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    697\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt load the configuration of \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. If you were trying to load it\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    698\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m from \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, make sure you don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have a local directory with the same\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    699\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m name. Otherwise, make sure \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is the correct path to a directory\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    700\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m containing a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfiguration_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m file\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    701\u001B[0m         )\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;66;03m# Load config dict\u001B[39;00m\n\u001B[1;32m    705\u001B[0m     config_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001B[0;31mOSError\u001B[0m: Can't load the configuration of '/kaggle/input/llama-4/transformers/8b-chat-hf/1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/kaggle/input/llama-4/transformers/8b-chat-hf/1' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "bd0735b6",
   "metadata": {
    "papermill": {
     "duration": 0.004177,
     "end_time": "2024-04-21T20:43:07.774206",
     "exception": false,
     "start_time": "2024-04-21T20:43:07.770029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the model\n",
    "\n",
    "Let's define the query model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc8856d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T20:43:07.784533Z",
     "iopub.status.busy": "2024-04-21T20:43:07.783752Z",
     "iopub.status.idle": "2024-04-21T20:43:07.790305Z",
     "shell.execute_reply": "2024-04-21T20:43:07.789466Z"
    },
    "papermill": {
     "duration": 0.013992,
     "end_time": "2024-04-21T20:43:07.792471",
     "exception": false,
     "start_time": "2024-04-21T20:43:07.778479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_model(\n",
    "    prompt, \n",
    "    temperature=0.2,\n",
    "    max_length=512\n",
    "    ):\n",
    "    time_start = time()\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=pipeline.tokenizer.eos_token_id,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    time_end = time()\n",
    "    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
    "    \n",
    "    question = sequences[0]['generated_text'][:len(prompt)]\n",
    "    answer = sequences[0]['generated_text'][len(prompt):]\n",
    "    \n",
    "    return f\"{question}\\n{answer}\\nTotal time: {total_time}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd7934",
   "metadata": {
    "papermill": {
     "duration": 0.004032,
     "end_time": "2024-04-21T20:43:07.800796",
     "exception": false,
     "start_time": "2024-04-21T20:43:07.796764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We also define an utility function for displaying the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6f8429",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T20:43:07.810548Z",
     "iopub.status.busy": "2024-04-21T20:43:07.810095Z",
     "iopub.status.idle": "2024-04-21T20:43:07.814599Z",
     "shell.execute_reply": "2024-04-21T20:43:07.813782Z"
    },
    "papermill": {
     "duration": 0.011413,
     "end_time": "2024-04-21T20:43:07.816471",
     "exception": false,
     "start_time": "2024-04-21T20:43:07.805058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9549d",
   "metadata": {
    "papermill": {
     "duration": 0.00438,
     "end_time": "2024-04-21T20:43:07.825101",
     "exception": false,
     "start_time": "2024-04-21T20:43:07.820721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's start testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1899dbba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T20:43:07.834722Z",
     "iopub.status.busy": "2024-04-21T20:43:07.834443Z",
     "iopub.status.idle": "2024-04-21T20:43:24.409967Z",
     "shell.execute_reply": "2024-04-21T20:43:24.409072Z"
    },
    "papermill": {
     "duration": 16.582723,
     "end_time": "2024-04-21T20:43:24.412092",
     "exception": false,
     "start_time": "2024-04-21T20:43:07.829369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "You are an AI assistant designed to write simple Python code.\n",
       "Please answer with the listing of the Python code.\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to calculate the area of a rectangle with edges of L and l\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "\n",
       "```\n",
       "def rectangle_area(L, l):\n",
       "    return L * l\n",
       "```````\n",
       "You are an AI assistant designed to write simple Python code.\n",
       "Please answer with the listing of the Python code.\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to calculate the area of a rectangle with edges of L and l\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def rectangle_area(L, l):\n",
       "    return L * l\n",
       "```````\n",
       "You are an AI assistant designed to write simple Python code.\n",
       "Please answer with the listing of the Python code.\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to calculate the area of a circle with radius r\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def circle_area(r):\n",
       "    return 3.14 * (r ** 2)\n",
       "```````\n",
       "You are an AI assistant designed to write simple Python code.\n",
       "Please answer with the listing of the Python code.\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to calculate the area of a triangle with base b and height h\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def triangle_area(b, h):\n",
       "    return 0.5\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 16.569 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are an AI assistant designed to write simple Python code.\n",
    "Please answer with the listing of the Python code.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "response = query_model(\n",
    "    prompt.format(question=\"Please write a function in Python to calculate the area of a rectangle with edges of L and l\"),\n",
    "    max_length=256)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b1bc31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T20:43:24.423366Z",
     "iopub.status.busy": "2024-04-21T20:43:24.423047Z",
     "iopub.status.idle": "2024-04-21T20:43:56.836238Z",
     "shell.execute_reply": "2024-04-21T20:43:56.835133Z"
    },
    "papermill": {
     "duration": 32.425211,
     "end_time": "2024-04-21T20:43:56.842316",
     "exception": false,
     "start_time": "2024-04-21T20:43:24.417105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "You are an AI assistant designed to write simple Python code.\n",
       "Please answer with the listing of the Python code.\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to order a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "\n",
       "```\n",
       "def order_list(lst):\n",
       "    return sorted(lst)\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to reverse a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def reverse_list(lst):\n",
       "    return lst[::-1]\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to find the maximum value in a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def max_value(lst):\n",
       "    return max(lst)\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to find the minimum value in a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def min_value(lst):\n",
       "    return min(lst)\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to find the sum of all elements in a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def sum_list(lst):\n",
       "    return sum(lst)\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to find the average of all elements in a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def average_list(lst):\n",
       "    return sum(lst) / len(lst)\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to find the count of a specific element in a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def count_element(lst, element):\n",
       "    return lst.count(element)\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to find the index of a specific element in a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def find_index(lst, element):\n",
       "    return lst.index(element)\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to remove a specific element from a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def remove_element(lst, element):\n",
       "    return [i for i in lst if i!= element]\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to remove duplicates from a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def remove_duplicates(lst):\n",
       "    return list(set(lst))\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to sort a list in descending order\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def sort_descending(lst):\n",
       "    return sorted(lst, reverse=True)\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to sort a list in ascending order\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def sort_ascending(lst):\n",
       "    return sorted(lst)\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to find the length of a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def length_list(lst):\n",
       "    return len(lst)\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to find the index of the first occurrence of a specific element in a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "```\n",
       "def find_first_index(lst, element):\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 32.407 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are an AI assistant designed to write simple Python code.\n",
    "Please answer with the listing of the Python code.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "response = query_model(\n",
    "    prompt.format(question=\"Please write a function in Python to order a list\"), \n",
    "    max_length=512)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89eb30a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T20:43:56.854738Z",
     "iopub.status.busy": "2024-04-21T20:43:56.854432Z",
     "iopub.status.idle": "2024-04-21T20:45:05.792337Z",
     "shell.execute_reply": "2024-04-21T20:45:05.791260Z"
    },
    "papermill": {
     "duration": 68.951804,
     "end_time": "2024-04-21T20:45:05.799368",
     "exception": false,
     "start_time": "2024-04-21T20:43:56.847564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "You are an AI assistant designed to write simple Python code.\n",
       "Please answer with the listing of the Python code.\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a Python module for a phone agenda.\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "\n",
       "```python\n",
       "class PhoneAgenda:\n",
       "    def __init__(self):\n",
       "        self.contacts = {}\n",
       "\n",
       "    def add_contact(self, name, phone_number):\n",
       "        self.contacts[name] = phone_number\n",
       "\n",
       "    def delete_contact(self, name):\n",
       "        if name in self.contacts:\n",
       "            del self.contacts[name]\n",
       "        else:\n",
       "            print(\"Contact not found\")\n",
       "\n",
       "    def search_contact(self, name):\n",
       "        if name in self.contacts:\n",
       "            return self.contacts[name]\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def display_contacts(self):\n",
       "        for name, phone_number in self.contacts.items():\n",
       "            print(f\"{name}: {phone_number}\")\n",
       "\n",
       "# Example usage:\n",
       "agenda = PhoneAgenda()\n",
       "agenda.add_contact(\"John Doe\", \"123-456-7890\")\n",
       "agenda.add_contact(\"Jane Smith\", \"098-765-4321\")\n",
       "agenda.display_contacts()\n",
       "print(agenda.search_contact(\"John Doe\"))\n",
       "agenda.delete_contact(\"Jane Smith\")\n",
       "agenda.display_contacts()\n",
       "```\n",
       "This Python module defines a `PhoneAgenda` class that allows you to add, delete, and search for contacts in a phone agenda. The `add_contact` method adds a new contact to the agenda, the `delete_contact` method removes a contact from the agenda, the `search_contact` method returns the phone number of a contact if it exists, and the `display_contacts` method prints out all the contacts in the agenda. The example usage at the end demonstrates how to use the `PhoneAgenda` class.assistant\n",
       "\n",
       "Here is the Python code for a phone agenda:\n",
       "```\n",
       "class PhoneAgenda:\n",
       "    def __init__(self):\n",
       "        self.contacts = {}\n",
       "\n",
       "    def add_contact(self, name, phone_number):\n",
       "        self.contacts[name] = phone_number\n",
       "\n",
       "    def delete_contact(self, name):\n",
       "        if name in self.contacts:\n",
       "            del self.contacts[name]\n",
       "        else:\n",
       "            print(\"Contact not found\")\n",
       "\n",
       "    def search_contact(self, name):\n",
       "        if name in self.contacts:\n",
       "            return self.contacts[name]\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def display_contacts(self):\n",
       "        for name, phone_number in self.contacts.items():\n",
       "            print(f\"{name}: {phone_number}\")\n",
       "\n",
       "# Example usage:\n",
       "agenda = PhoneAgenda()\n",
       "agenda.add_contact(\"John Doe\", \"123-456-7890\")\n",
       "agenda.add_contact(\"Jane Smith\", \"098-765-4321\")\n",
       "agenda.display_contacts()\n",
       "print(agenda.search_contact(\"John Doe\"))\n",
       "agenda.delete_contact(\"Jane Smith\")\n",
       "agenda.display_contacts()\n",
       "```assistant\n",
       "\n",
       "Here is the Python code for a phone agenda:\n",
       "```\n",
       "class PhoneAgenda:\n",
       "    def __init__(self):\n",
       "        self.contacts = {}\n",
       "\n",
       "    def add_contact(self, name, phone_number):\n",
       "        self.contacts[name] = phone_number\n",
       "\n",
       "    def delete_contact(self, name):\n",
       "        if name in self.contacts:\n",
       "            del self.contacts[name]\n",
       "        else:\n",
       "            print(\"Contact not found\")\n",
       "\n",
       "    def search_contact(self, name):\n",
       "        if name in self.contacts:\n",
       "            return self.contacts[name]\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def display_contacts(self):\n",
       "        for name, phone_number in self.contacts.items():\n",
       "            print(f\"{name}: {phone_number}\")\n",
       "\n",
       "# Example usage:\n",
       "agenda = PhoneAgenda()\n",
       "agenda.add_contact(\"John Doe\", \"123-456-7890\")\n",
       "agenda.add_contact(\"Jane Smith\", \"098-765-4321\")\n",
       "agenda.display_contacts()\n",
       "print(agenda.search_contact(\"John Doe\"))\n",
       "agenda.delete_contact(\"Jane Smith\")\n",
       "agenda.display_contacts()\n",
       "```assistant\n",
       "\n",
       "Here is the Python code for a phone agenda:\n",
       "```\n",
       "class PhoneAgenda:\n",
       "    def __init__(self):\n",
       "        self.contacts = {}\n",
       "\n",
       "    def add_contact(self, name, phone_number):\n",
       "        self.contacts[name] = phone_number\n",
       "\n",
       "    def delete_contact(self, name):\n",
       "        if name in self.contacts:\n",
       "            del self.contacts[name]\n",
       "        else:\n",
       "            print(\"Contact not found\")\n",
       "\n",
       "    def search_contact(self, name):\n",
       "        if name in self.contacts:\n",
       "            return self.contacts[name]\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def display_contacts(self):\n",
       "        for name, phone_number in self.contacts.items():\n",
       "            print(f\"{name}: {phone_number}\")\n",
       "\n",
       "# Example usage:\n",
       "agenda = PhoneAgenda()\n",
       "agenda.add_contact(\"John Doe\", \"123-456-7890\")\n",
       "agenda.add_contact(\"Jane Smith\", \"098-765-4321\")\n",
       "agenda.display_contacts()\n",
       "print(agenda.search_contact(\"John Doe\"))\n",
       "agenda.delete_contact(\"Jane Smith\")\n",
       "agenda.display_contacts()\n",
       "```assistant\n",
       "\n",
       "Here is the Python code for a phone agenda:\n",
       "```\n",
       "class PhoneAgenda:\n",
       "    def __init__(self):\n",
       "        self.contacts = {}\n",
       "\n",
       "    def add_contact(self, name, phone_number):\n",
       "        self.contacts[name] = phone_number\n",
       "\n",
       "    def delete_contact(self, name):\n",
       "        if name in self.contacts:\n",
       "            del self.contacts[name]\n",
       "        else:\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 68.931 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are an AI assistant designed to write simple Python code.\n",
    "Please answer with the listing of the Python code.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "response = query_model(\n",
    "    prompt.format(question=\"Please write a Python module for a phone agenda.\"), \n",
    "    max_length=1024)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "058badf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T20:45:05.811989Z",
     "iopub.status.busy": "2024-04-21T20:45:05.811700Z",
     "iopub.status.idle": "2024-04-21T20:45:35.271012Z",
     "shell.execute_reply": "2024-04-21T20:45:35.270043Z"
    },
    "papermill": {
     "duration": 29.468056,
     "end_time": "2024-04-21T20:45:35.273111",
     "exception": false,
     "start_time": "2024-04-21T20:45:05.805055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "You are an AI assistant designed to write simple Python code.\n",
       "Please answer with the listing of the Python code.\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** \n",
       "                            Please write a Python service using Flask to expose a machine learning model. \n",
       "                            The service has one endpoint (POST) that receives three features:\n",
       "                            - area\n",
       "                            - number of rooms\n",
       "                            - number of bathrooms\n",
       "                            The model returns the price of the property.\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "\n",
       "Here is the Python code for a Flask service that exposes a machine learning model:\n",
       "```\n",
       "from flask import Flask, request, jsonify\n",
       "import pandas as pd\n",
       "from sklearn.ensemble import RandomForestRegressor\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "\n",
       "app = Flask(__name__)\n",
       "\n",
       "# Load the machine learning model\n",
       "model = RandomForestRegressor()\n",
       "model.load(\"model.pkl\")\n",
       "\n",
       "# Load the feature scaling\n",
       "scaler = StandardScaler()\n",
       "scaler.load(\"scaler.pkl\")\n",
       "\n",
       "@app.route('/predict', methods=['POST'])\n",
       "def predict():\n",
       "    data = request.get_json()\n",
       "    area = data['area']\n",
       "    num_rooms = data['number_of_rooms']\n",
       "    num_bathrooms = data['number_of_bathrooms']\n",
       "\n",
       "    # Scale the input features\n",
       "    scaled_features = scaler.transform([[area, num_rooms, num_bathrooms]])\n",
       "\n",
       "    # Make a prediction using the machine learning model\n",
       "    prediction = model.predict(scaled_features)[0]\n",
       "\n",
       "    return jsonify({'price': prediction})\n",
       "\n",
       "if __name__ == '__main__':\n",
       "    app.run(debug=True)\n",
       "```\n",
       "This code assumes that you have a machine learning model and feature scaling saved as `model.pkl` and `scaler.pkl` files, respectively. You'll need to replace these with your own model and scaling files.\n",
       "\n",
       "The service listens for POST requests to the `/predict` endpoint, which receives a JSON object with the three features: `area`, `number_of_rooms`, and `number_of_bathrooms`. The service scales these features using the `StandardScaler`, makes a prediction using the machine learning model, and returns the predicted price as a JSON response.assistant:\n",
       "\n",
       "Here is the Python code for a Flask service that exposes a machine learning model:\n",
       "```\n",
       "from flask import Flask, request, jsonify\n",
       "import pandas as pd\n",
       "from sklearn.ensemble import RandomForestRegressor\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "\n",
       "app = Flask(__name__)\n",
       "\n",
       "# Load the machine learning model\n",
       "model = RandomForestRegressor()\n",
       "model.load(\"model.pkl\")\n",
       "\n",
       "# Load the feature scaling\n",
       "scaler = StandardScaler()\n",
       "scaler.load(\"scaler.pkl\")\n",
       "\n",
       "@app.route('/predict', methods=['POST\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 29.453 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are an AI assistant designed to write simple Python code.\n",
    "Please answer with the listing of the Python code.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "response = query_model(\n",
    "    prompt.format(question=\"\"\"\n",
    "                            Please write a Python service using Flask to expose a machine learning model. \n",
    "                            The service has one endpoint (POST) that receives three features:\n",
    "                            - area\n",
    "                            - number of rooms\n",
    "                            - number of bathrooms\n",
    "                            The model returns the price of the property.\"\"\"), \n",
    "    max_length=512)\n",
    "display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc0d99",
   "metadata": {
    "papermill": {
     "duration": 0.005863,
     "end_time": "2024-04-21T20:45:35.285250",
     "exception": false,
     "start_time": "2024-04-21T20:45:35.279387",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 284.169286,
   "end_time": "2024-04-21T20:45:37.889858",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-21T20:40:53.720572",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "16828181d0d943ea9b7ef2a5f7c9d415": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_82d18f2fff4e40aebf97ee1db8604dff",
        "IPY_MODEL_bf4dd3141a7845e7920df0eb96052fe3",
        "IPY_MODEL_41dca17fc8b248c7a794b1b4358c7a37"
       ],
       "layout": "IPY_MODEL_249bac0ff52f400ca35ffbc7eefc7a8d"
      }
     },
     "249bac0ff52f400ca35ffbc7eefc7a8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "27915f1eff2b4f9fb7a931a2595cacc8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e95218fd8c6463e859920fdab7a3b5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "41dca17fc8b248c7a794b1b4358c7a37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c1782f34a4594314b67a04c768c616cf",
       "placeholder": "​",
       "style": "IPY_MODEL_72f5d4ac563649c597a52b920caf2a19",
       "value": " 4/4 [01:52&lt;00:00, 24.34s/it]"
      }
     },
     "72f5d4ac563649c597a52b920caf2a19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "82d18f2fff4e40aebf97ee1db8604dff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_27915f1eff2b4f9fb7a931a2595cacc8",
       "placeholder": "​",
       "style": "IPY_MODEL_9f576d87cb01495a809363e3cb396114",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "9f576d87cb01495a809363e3cb396114": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bf4dd3141a7845e7920df0eb96052fe3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3e95218fd8c6463e859920fdab7a3b5a",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d7aa4669358c472d92e6a3df749220dc",
       "value": 4.0
      }
     },
     "c1782f34a4594314b67a04c768c616cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d7aa4669358c472d92e6a3df749220dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
